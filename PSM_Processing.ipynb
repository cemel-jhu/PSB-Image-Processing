{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" PSM_Processing.ipynb\n",
    "\n",
    "This notebook layout the processing pipeline for analysing the propagation\n",
    "speed, and embrynic width of PSMs.\n",
    "\n",
    "Note: This notebook is exposed as a module for larger scale parameter studies.\n",
    "It was decided not to refactor the notebook into modules, as the defintion of\n",
    "the functions provides the motivation and story for processing PSMs. For an\n",
    "example of how this might be used in larger scale instances, refer to\n",
    "`parameter_study.py`\"\"\"\n",
    "\n",
    "%load_ext notebook_helpers\n",
    "imported = not __name__ == \"__main__\"\n",
    "## use %load_ext nb_black, to run the yapf chromium formatter\n",
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "id": "6I04FGZmhFOG",
    "outputId": "85651756-275f-4473-f917-a9666f56866c",
    "scrolled": false,
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "from PSMProcessing import ProcessContext\n",
    "from PSMProcessing.geometry import score_profile, convex_hull\n",
    "\n",
    "import glob\n",
    "from collections import OrderedDict, namedtuple\n",
    "import warnings\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from skimage import feature as ski_feature\n",
    "from skimage.transform import resize as rs\n",
    "import imageio\n",
    "\n",
    "from scipy.stats import probplot, norm\n",
    "import pylab\n",
    "\n",
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-mM_WLZL82b"
   },
   "outputs": [],
   "source": [
    "\"\"\"Global definitions and variables for a given run.\"\"\"\n",
    "\n",
    "# Global cache of images and data, initialized to be empty here..\n",
    "images = {}\n",
    "cube = np.array([[[]]])\n",
    "times = []\n",
    "\n",
    "# Run specific variables\n",
    "experiment = (\n",
    "    32\n",
    ")  # The experiment number. Images for this experiment should be in folder {experiment}/\n",
    "band = 8  # The band to examine within the experiment\n",
    "export_gifs = True  # Whether gifs should be exported\n",
    "interactive = True  # Whether interactive parts of the script should be run.\n",
    "recompute = False  # Forces recomputation. Warning, will potentially overwrite\n",
    "# other variables.\n",
    "\n",
    "# Experiment specific variables. These will be overwritten\n",
    "# if the experiment has already been analyzed.\n",
    "scale = 768 / 40.27  # Pixels to um ratio.\n",
    "frequency = 75  # The frequency of loading\n",
    "direction = 2.0 / 3  # The slope of the PSBs in the experiment relative to the image.\n",
    "# Seemingly allows for margin of error.\n",
    "beam_height = 600  # In pixels\n",
    "beam_width = 180  # in pixels\n",
    "flip = True  # All PSB slope should be positive. If the slope is negative, a\n",
    "# reflection is performed on the picture.\n",
    "\n",
    "# Band specific variables\n",
    "start = 0  # The start offset of a given band.\n",
    "width = beam_height  # The width of the given band.\n",
    "\n",
    "# New experiment specific variables. These values will be ignored\n",
    "# unless no records exist for the given variable.\n",
    "# Image loading specific variables.\n",
    "resize = 0  # Scale factor by which the image should be resized. Set 0 for no resizing.\n",
    "borders = [50, 140, 30, 40]  # Left, Right, Top, Bottom.\n",
    "# Cropping margins for the processed images.\n",
    "margin_x = [20, 3]  # The margin to provide a partially cropped beam,\n",
    "# when determining how to crop the height\n",
    "\n",
    "# Event capture specific variables.\n",
    "time_margin = 15  # How much of the video to ignore at the beginning and end\n",
    "sigma = 0.65  # Smoothing deviation for filtering procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load records if possible.\n",
    "Record = namedtuple(\n",
    "    \"Record\",\n",
    "    (\"scale frequency direction sigma \"\n",
    "     \"beam_height beam_width flip time_margin bands\"),\n",
    ")\n",
    "Band = namedtuple(\"Band\", (\"start width lower upper\"))\n",
    "\n",
    "# Globals associated with each experiment analysis.\n",
    "# Update this variable accordingly with each analysis performed.\n",
    "# The currently set data is particular to our experimental data.\n",
    "experiments = {\n",
    "    32:\n",
    "        Record(\n",
    "            scale=768.0 / 42.4,\n",
    "            frequency=75,\n",
    "            direction=2.0 / 3,\n",
    "            sigma=0.65,\n",
    "            beam_height=575,\n",
    "            beam_width=270,\n",
    "            flip=False,\n",
    "            time_margin=15,\n",
    "            bands={\n",
    "                2: Band(195, 35, 2.5, 9.5),\n",
    "                3: Band(252, 30, 3, 7.5),\n",
    "                5: Band(315, 30, 2.5, 7.5),\n",
    "                8: Band(390, 30, 8, 12.5),\n",
    "            },\n",
    "        ),\n",
    "    24:\n",
    "        Record(\n",
    "            scale=768.0 / 45.43,\n",
    "            frequency=75,\n",
    "            direction=0.5,\n",
    "            sigma=0.65,\n",
    "            beam_height=500,\n",
    "            beam_width=200,\n",
    "            flip=True,\n",
    "            time_margin=60,\n",
    "            bands={\n",
    "                2: Band(480, 25, 9, 11),\n",
    "                3: Band(465, 20, 0, 0),\n",
    "                4: Band(440, 25, 0, 0),\n",
    "                5: Band(412, 25, 0, 0),\n",
    "                6: Band(390, 20, 8, 9.8),\n",
    "                7: Band(365, 25, 3, 11),\n",
    "                8: Band(345, 20, 1, 6),\n",
    "            },\n",
    "        ),\n",
    "    25:\n",
    "        Record(\n",
    "            scale=768.0 / 40.27,\n",
    "            frequency=75,\n",
    "            direction=2.0 / 3,\n",
    "            sigma=0.65,\n",
    "            beam_height=600,\n",
    "            beam_width=180,\n",
    "            flip=True,\n",
    "            time_margin=1,\n",
    "            bands={\n",
    "                7: Band(265, 35, 0, 0),\n",
    "                8: Band(210, 35, 0, 0),\n",
    "                9: Band(185, 25, 0, 0)\n",
    "            },\n",
    "        ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dylan/results/virtualenv/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Overriding definitions for records.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "%%skip $imported\n",
    "# If the experiment is listed, we use the values specified by the `experiments` variable\n",
    "if experiment in experiments:\n",
    "  warnings.warn(\"Overriding definitions for records.\")\n",
    "  (scale, frequency, direction, sigma,\n",
    "   beam_height, beam_width, flip, time_margin, bands) = experiments[experiment]\n",
    "  if band in bands:\n",
    "    start, width, lower, upper = bands[band]\n",
    "  else:\n",
    "    warnings.warn((\"Band is not defined. Considering whole beam. \"\n",
    "                  \" Please redefine `start, width` for a given band.\"))    \n",
    "else:\n",
    "  warnings.warn(\"Could not find record. Relying on global defintions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_images(\n",
    "    src,\n",
    "    borders=borders,\n",
    "    resize=resize,\n",
    "    margin_x=margin_x,\n",
    "    frequency=frequency,\n",
    "    beam_width=beam_width,\n",
    "    beam_height=beam_height,\n",
    "):\n",
    "  \"\"\"Extracts a PSB from an image based on lighting hueristics.\n",
    "  \n",
    "  Given a beam height and size, determines where the beam is most\n",
    "  likely to be based on dramaticlighting changes from the background\n",
    "  to foreground.\n",
    "  \n",
    "  Args:\n",
    "      src: A file name for an expected to be of the form \"path/to/image/image_timestamp.bmp\" \n",
    "      borders: The cropping margins to apply to the image\n",
    "      resize: The scale factor to resize the image.\n",
    "      margin_x: Margins by which \n",
    "\n",
    "  Returns:\n",
    "      A tuple of (cycle #, PSB of fixed size (beam_width x beam_height))\n",
    "  \"\"\"\n",
    "  cycle = int(\n",
    "      src.split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]) / 1000.0 * frequency\n",
    "  image = imageio.imread(src)\n",
    "\n",
    "  # Go from RGB -> blackwhite and cast\n",
    "  image = np.rint(np.mean(image, axis=-1)).astype(np.uint8)\n",
    "\n",
    "  # Resize using skimage.\n",
    "  if resize:\n",
    "    image = rs(\n",
    "        image, tuple(i * resize for i in image.shape), anti_aliasing=True)\n",
    "\n",
    "  # Crop borders form image\n",
    "  image = image[borders[0]:-borders[1], borders[2]:-borders[3]]\n",
    "\n",
    "  # Pretty reliably finds crop position where the beam occurs.\n",
    "  # could do convolution like for the height.\n",
    "  x = np.sum(scipy.ndimage.filters.gaussian_filter(image, 3), axis=0)\n",
    "  crop_x = np.argmax(np.convolve(x.T, np.ones(beam_width, dtype=int), \"valid\"))\n",
    "\n",
    "  # Cropping for height is a little more unstable. As such, smoothing and crop\n",
    "  # margins were found to help.\n",
    "  smoothed = scipy.ndimage.filters.gaussian_filter(image, 3)\n",
    "  smoothed = smoothed[:, crop_x - margin_x[0]:crop_x + margin_x[1]]\n",
    "\n",
    "  # Attempts find crop position from the top of the beam\n",
    "  y = np.sum(smoothed, axis=1)\n",
    "  crop_y = np.argmin(np.convolve(y.T, np.ones(beam_height, dtype=int), \"valid\"))\n",
    "\n",
    "  # Apply the crop with a fixed size\n",
    "  return (cycle, image[crop_y:crop_y + beam_height, crop_x:crop_x + beam_width])\n",
    "\n",
    "\n",
    "def load_images(path=\"images\",\n",
    "                experiment=experiment,\n",
    "                bmps=\"images/{}/*.bmp\",\n",
    "                recompute=False,\n",
    "                **kwargs):\n",
    "  try:\n",
    "    if recompute:\n",
    "      raise\n",
    "    images = np.load(\n",
    "        \"npy/{}_{}.npy\".format(path, experiment), allow_pickle=True).item()\n",
    "  except:\n",
    "    pool = multiprocessing.Pool()\n",
    "    images = OrderedDict(\n",
    "        sorted(\n",
    "            pool.map(\n",
    "                ProcessContext(extract_images, **kwargs),\n",
    "                glob.glob(bmps.format(experiment)),\n",
    "            )))\n",
    "    pool.close()\n",
    "    del pool\n",
    "    np.save(\"npy/{}_{}\".format(path, experiment), images)\n",
    "  return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $imported\n",
    "# Extract or load the images relevant to the experiment.\n",
    "images = load_images(recompute=recompute)\n",
    "beam_height, beam_width = images[list(images.keys())[0]].shape\n",
    "times = list(images.keys())\n",
    "image = images[times[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WHfhUpQ1SJpX"
   },
   "outputs": [],
   "source": [
    "\"\"\"Export a gif of the newly cropped images.\"\"\"\n",
    "if export_gifs and images:\n",
    "  gif = \"psb_stable_{}.gif\".format(experiment)\n",
    "\n",
    "  # We can smooth through time too!\n",
    "  imageio.mimsave(\n",
    "      gif,\n",
    "      scipy.ndimage.filters.gaussian_filter(list(images.values()), 1),\n",
    "      fps=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "yO1Hs8OP8dEZ",
    "outputId": "644adb8a-a517-4095-ca56-dc71e08fb89c"
   },
   "outputs": [],
   "source": [
    "\"\"\"Examine the last frame of our image.\"\"\"\n",
    "if interactive and not imported:\n",
    "  plt.imshow(scipy.ndimage.filters.gaussian_filter(image, 2))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 791
    },
    "colab_type": "code",
    "id": "DDA5AHIyEKGY",
    "outputId": "7254ee74-2576-49c1-a163-b9ef4f33b3e9"
   },
   "outputs": [],
   "source": [
    "def determine_psb_slip_direction(image, sigma=1):\n",
    "  \"\"\"Run a gradient on the image for a ball park of the slip direction.\n",
    "  \n",
    "  Depending on how the gradient get calculated, the answer might be inverse to\n",
    "  the gradient.\n",
    "  \n",
    "  Args:\n",
    "    image: The final image in the set (has the most psbs)\n",
    "    sigma: The amount of smoothing to do on the image prior to running gradient\n",
    "  Returns: Image of gradients\n",
    "  \"\"\"\n",
    "  filtered = scipy.ndimage.filters.gaussian_filter(image, sigma)\n",
    "  gx, gy = np.gradient(filtered)\n",
    "  return np.arctan2(gy, gx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $imported\n",
    "plt.close(\"all\")\n",
    "img = determine_psb_slip_direction(image, sigma=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Examine gradients in image to quantatively examine the slip direction with\n",
    "reference to the image.\"\"\"\n",
    "if interactive and not imported:\n",
    "  plt.figure()\n",
    "  plt.title(\"Line directions within the image\")\n",
    "  plt.imshow(img)\n",
    "  plt.colorbar()\n",
    "  # Run a histogram on the gradients to see what's most likely.\n",
    "  plt.show()\n",
    "  fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "  axs[0].set_title(\"Histograms of line directions\")\n",
    "  axs[0].set_xlabel(\"Line direction values\")\n",
    "  axs[0].set_ylabel(\"Frequency\")\n",
    "  axs[0].hist(img)\n",
    "  # Histogram of general intensities\n",
    "  axs[1].set_title(\"Histograms of image intensities\")\n",
    "  axs[1].set_xlabel(\"Image internsities\")\n",
    "  axs[1].set_ylabel(\"Frequency\")\n",
    "  axs[1].hist(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7R7hhf-FrWN"
   },
   "outputs": [],
   "source": [
    "\"\"\"Take direction from histogram if not already suggested.\"\"\"\n",
    "if not direction:\n",
    "    direction = np.mean(img[np.all([img > 0.5, img < 1.0], axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "WgBk00yVGYKu",
    "outputId": "f4f777af-1ead-45ca-b4de-2f7a023e4a55"
   },
   "outputs": [],
   "source": [
    "\"\"\"Reorient our image onto the PSB direction.\"\"\"\n",
    "\n",
    "\n",
    "def extract_length(image_shape, direction=1):\n",
    "    \"\"\"Grab the lines along the slip direction for the entire image.\n",
    "  \n",
    "  Args:\n",
    "    image_shape: shape of the images, should be [x, y, 1].\n",
    "    direction: The direction of the slip plane we determined.\n",
    "    \n",
    "  Returns:\n",
    "    A tuple of (line lengths, line directions)\n",
    "  \"\"\"\n",
    "\n",
    "    sample_count = min(image_shape[:2])\n",
    "    gradient = direction\n",
    "    norm = np.linalg.norm\n",
    "\n",
    "    lengths = []\n",
    "    lins = []\n",
    "    X = 0\n",
    "    # Scan along the y axis\n",
    "    for Y in range(image_shape[0]):\n",
    "        # Determine whether line will be flush to the right or top of the image.\n",
    "        if X + Y / gradient > image_shape[1]:\n",
    "            n = norm([X - image_shape[1], image_shape[1] * gradient])\n",
    "            x = np.linspace(X, image_shape[1], sample_count)\n",
    "            y = np.linspace(Y, -image_shape[1] * gradient + Y, sample_count)\n",
    "        else:\n",
    "            n = norm([Y / gradient, Y])\n",
    "            x = np.linspace(0, Y / gradient, sample_count)\n",
    "            y = np.linspace(Y, 0, sample_count)\n",
    "        lengths.append(n)\n",
    "        lins.append(np.vstack((y, x)))\n",
    "\n",
    "    # Keeps scanning down the y axis, but now the line starts at the bottom of\n",
    "    # the image. End when line no longer is on the image.\n",
    "    while -image_shape[1] * gradient + Y + X < image_shape[0]:\n",
    "        # Determine whether line will be flush to the right or top of the image.\n",
    "        if X + Y / gradient > image_shape[1]:\n",
    "            n = norm([X / gradient - image_shape[1], image_shape[1] * gradient - X])\n",
    "            x = np.linspace(X / gradient, image_shape[1], sample_count)\n",
    "            y = np.linspace(Y, -image_shape[1] * gradient + Y + X, sample_count)\n",
    "        else:\n",
    "            n = norm([X / gradient - (X + Y) / gradient, Y])\n",
    "            x = np.linspace(X / gradient, (X + Y) / gradient, sample_count)\n",
    "            y = np.linspace(Y, 0, sample_count)\n",
    "        X += 1\n",
    "        lengths.append(n)\n",
    "        lins.append(np.vstack((y, x)))\n",
    "\n",
    "    return np.array(lengths), np.array(lins)\n",
    "\n",
    "\n",
    "def extract(image, lins, flip=flip):\n",
    "    \"\"\"Apply calculated lines to image.\"\"\"\n",
    "    result = []\n",
    "    if flip:\n",
    "        image = np.fliplr(np.flip(image))\n",
    "    for stack in lins:\n",
    "        # Extract the values along the line, using cubic interpolation\n",
    "        result.append(scipy.ndimage.map_coordinates(image, stack))\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "if not imported:\n",
    "    lengths, lins = extract_length(image.shape, direction=direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Validate that the images are properly sheared.\"\"\"\n",
    "if interactive and not imported:\n",
    "    plt.imshow(extract(image, lins, flip=flip))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkG-4LfCC-gf"
   },
   "outputs": [],
   "source": [
    "\"\"\"Grab a particular PSB\"\"\"\n",
    "\n",
    "# Ideally, this would be a lambda, but `processing` pickles the functions\n",
    "# in map, thus the function must be global.\n",
    "def _shear_images_helper(item, lins=None, **kwargs):\n",
    "    key, image = item\n",
    "    return (key, extract(image, lins, **kwargs))\n",
    "\n",
    "\n",
    "def shear_images(\n",
    "    images, recompute=False, path=\"extracted\", experiment=experiment, **kwargs\n",
    "):\n",
    "    try:\n",
    "        if recompute:\n",
    "            raise\n",
    "        extracted = np.load(\n",
    "            \"npy/{}_{}.npy\".format(path, experiment), allow_pickle=True\n",
    "        ).item()\n",
    "    except:\n",
    "        pool = multiprocessing.Pool()\n",
    "        extracted = OrderedDict(\n",
    "            sorted(\n",
    "                pool.map(ProcessContext(_shear_images_helper, **kwargs), images.items())\n",
    "            )\n",
    "        )\n",
    "        pool.close()\n",
    "        del pool\n",
    "        np.save(\"npy/{}_{}\".format(path, experiment), extracted)\n",
    "    return extracted\n",
    "\n",
    "\n",
    "def clean(cube, res=200, blur=True, sigma=2):\n",
    "    dim = min([cube.shape[0] // res, cube.shape[1] // res])\n",
    "    scale = (dim, dim, cube.shape[2] // res)\n",
    "    result = scipy.ndimage.morphology.grey_opening(\n",
    "        scipy.ndimage.morphology.grey_closing(cube, scale), scale\n",
    "    )\n",
    "    if blur:\n",
    "        result = scipy.ndimage.filters.gaussian_filter(cube, sigma)\n",
    "    return result\n",
    "\n",
    "\n",
    "if not imported:\n",
    "    cube = np.array(\n",
    "        list(shear_images(images, recompute=recompute, flip=flip, lins=lins).values())\n",
    "    ).astype(float)\n",
    "    cube = clean(cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Examine the the last frame of our sheared time cube.\"\"\"\n",
    "if interactive and not imported:\n",
    "    plt.imshow(cube[-1, :, :])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGF-tP2_EJqT"
   },
   "outputs": [],
   "source": [
    "\"\"\"Export a gif of the sheared time cube.\"\"\"\n",
    "if export_gifs and cube.size > 0:\n",
    "    gif = \"psb_stable_axis_{}.gif\".format(experiment)\n",
    "    imageio.mimsave(gif, cube.astype(\"uint8\"), fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1052
    },
    "colab_type": "code",
    "id": "pvpO7tAcEO9e",
    "outputId": "35381dea-f033-441c-e47a-9a0d53d26be5"
   },
   "outputs": [],
   "source": [
    "\"\"\"Examine intensity profiles over time. This is useful for developing event\n",
    "determination algorithms.\"\"\"\n",
    "if interactive and not imported:\n",
    "    # Checkout our time cube and various slice, in time/space\n",
    "    offset = 0\n",
    "    s = 405\n",
    "    p = 150\n",
    "\n",
    "    avg_profile = np.average(cube[:time_margin, s, :], axis=0)\n",
    "    smoothed = scipy.ndimage.filters.gaussian_filter(np.abs(cube[:, s, :]), 1)\n",
    "    smoothed_difference = np.abs(\n",
    "        scipy.ndimage.filters.gaussian_filter(cube[:, s, :] - avg_profile, 1)\n",
    "    )\n",
    "\n",
    "    # t = 180 in ex. 32 is a good example of little change\n",
    "    plt.title(\n",
    "        \"Smoothed intensity of pixel {} of slice {} along time.\".format(p, offset + s)\n",
    "    )\n",
    "    plt.plot(scipy.ndimage.filters.gaussian_filter(smoothed_difference[:, p], 9))\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Time domain image of slice {}.\".format(offset + s))\n",
    "    plt.imshow(smoothed)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Differences in time domain image of slice {}.\".format(offset + s))\n",
    "    plt.imshow(smoothed_difference)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 707
    },
    "colab_type": "code",
    "id": "WWvTvFCIGksG",
    "outputId": "db0ac331-fe00-405e-d84c-a97e856c3998"
   },
   "outputs": [],
   "source": [
    "\"\"\"Methods to extract the events from each time slice.\"\"\"\n",
    "\n",
    "\n",
    "def determine_event_by_KL(time_slice, margin=time_margin):\n",
    "    \"\"\"Determine the cycle at which an 'event' occurs, by comparing the\n",
    "  distributions about a point using KL divergence.\"\"\"\n",
    "    # Works best for a binary distribution over the time series distributio\n",
    "    s = []\n",
    "    for pivot in range(margin, len(time_slice) - margin):\n",
    "        sP = np.std(time_slice[:pivot]) + 1e-5\n",
    "        sQ = np.std(time_slice[pivot:]) + 1e-5\n",
    "        P = np.mean(time_slice[:pivot])\n",
    "        Q = np.mean(time_slice[pivot:])\n",
    "        s.append(\n",
    "            np.abs(np.log(sP / sQ) + (sQ ** 2 + (P - Q) ** 2) / (2 * sP ** 2) - 0.5)\n",
    "        )\n",
    "\n",
    "    return margin + np.argmax(s)\n",
    "\n",
    "\n",
    "def determine_event_by_Otsu(time_slice, pool, margin=time_margin):\n",
    "    \"\"\"Determine the cycle at which an 'event' occurs. This dramatic change in signal\n",
    "  should indicate a PSB propagating at this time.\n",
    "  \n",
    "  We note that in the case of a continuous series such as the intensity of a\n",
    "  pixel through time, this works by finding the minimum point at which the standard\n",
    "  deviations differ. The idea being that each segment will have low class variance,\n",
    "  and as such, the difference of the deviations should be minimized.\n",
    "  Not this is not robust to lighting changes over time.\n",
    "  \n",
    "  In the case of a discrete series, i.e. post canny-processing. We expect the frequency\n",
    "  of edge detections to increase after the event. Thus the process works by finding the\n",
    "  maximum spot at which this appears to occur.\n",
    "  \"\"\"\n",
    "    # Some what well studied. My current stablization is a little noisy, so not\n",
    "    # great, but has the potential to be better.\n",
    "    # This event determination is a little bit of a hack for now. To revisit.\n",
    "    # https://en.wikipedia.org/wiki/Step_detection\n",
    "    s = []\n",
    "    for pivot in range(margin, len(time_slice) - margin):\n",
    "        s.append(np.abs(np.std(time_slice[:pivot]) - np.std(time_slice[pivot:])))\n",
    "    return margin + pool(s)\n",
    "\n",
    "\n",
    "def velocity_profile(\n",
    "    S,\n",
    "    cube=cube,\n",
    "    times=times,\n",
    "    sigma=sigma,\n",
    "    scale=scale,\n",
    "    canny=True,\n",
    "    kl=True,\n",
    "    length=None,\n",
    "    lengths=None,\n",
    "    time_margin=time_margin,\n",
    "    offset=0,\n",
    "):\n",
    "    \"\"\"Grab all 'events' for a given slice.\n",
    "  \n",
    "  Args:\n",
    "    S: the slice index to look at\n",
    "    sigma: Smooothing parameter prior to event detection. High smoothing\n",
    "      actually produces some nicer contours.\n",
    "  \n",
    "  \"\"\"\n",
    "    time_margin = max(time_margin, 1)\n",
    "    # We 0 center about the noise from t < 0\n",
    "    cut = cube[:, S, :] - np.mean(cube[:time_margin, S, :], axis=0)\n",
    "    if canny:\n",
    "        cut = ski_feature.canny(cut, sigma=sigma)\n",
    "        pool = np.argmax\n",
    "    else:\n",
    "        cut = scipy.ndimage.filters.gaussian_filter(cut, sigma)\n",
    "        pool = np.argmin\n",
    "\n",
    "    determine_event = determine_event_by_KL\n",
    "    if not kl:\n",
    "        determine_event = lambda s, **kwargs: determine_event_by_Otsu(s, pool, **kwargs)\n",
    "\n",
    "    if not length:\n",
    "        if lengths is None:\n",
    "            raise Exception(\"Length not specified\")\n",
    "        length = lengths[S + offset]\n",
    "    k = scale / length\n",
    "\n",
    "    events = []\n",
    "    scaled_events = []\n",
    "    for i in range(cut.shape[-1]):\n",
    "        events.append((i, determine_event(cut[:, i], margin=time_margin)))\n",
    "        scaled_events.append((k * i, times[events[-1][1]]))\n",
    "\n",
    "    return list(zip(*scaled_events)), list(zip(*events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Examine profiles for select slice.\"\"\"\n",
    "if interactive and not imported:\n",
    "    scaled_profile, profile = velocity_profile(s, sigma=sigma, cube=cube, lengths=lengths)\n",
    "    plt.title(\"Position, event of a single slice, unscaled, overlayed.\")\n",
    "    plt.plot(*profile)\n",
    "    plt.imshow(smoothed)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IT3xT2YIT7T"
   },
   "outputs": [],
   "source": [
    "\"\"\"Grab events for entire PSB\"\"\"\n",
    "\n",
    "\n",
    "def extract_events(cube, recompute=False, path=\"vs\", experiment=experiment, **kwargs):\n",
    "    try:\n",
    "        if recompute:\n",
    "            raise\n",
    "        vs = np.load(\"npy/{}_{}.npy\".format(path, experiment), allow_pickle=True)\n",
    "    except:\n",
    "        kwargs[\"cube\"] = cube\n",
    "        pool = multiprocessing.Pool()\n",
    "        vs = pool.map(ProcessContext(velocity_profile, **kwargs), range(cube.shape[1]))\n",
    "        pool.close()\n",
    "        del pool\n",
    "        vs = np.array(list(zip(*vs)))\n",
    "        np.save(\"npy/{}_{}\".format(path, experiment), vs)\n",
    "    return vs\n",
    "\n",
    "\n",
    "if not imported:\n",
    "    vs = extract_events(cube, recompute=recompute, path=\"vs\", lengths=lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Choose a particular psb from the sheared images.\"\"\"\n",
    "if interactive and not imported:\n",
    "    # Check out our slice\n",
    "\n",
    "    ## Play with cropping values here, and record in experiments\n",
    "    ## definition in cell #3\n",
    "    # width = 30\n",
    "    # start =  390\n",
    "\n",
    "    ## It may also be desireable to overwrite the band # here for quick\n",
    "    ## iteration.\n",
    "    # band = 1\n",
    "\n",
    "    # plt.imshow(cube[-1, start:start + width, :])\n",
    "    plt.imshow(cube[-1, start : start + width])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Massage and filter data for extreme outliers.\"\"\"\n",
    "\n",
    "\n",
    "def split_vs(vs, start=start, width=width):\n",
    "    # Parse vs into scaled and unscaled parts.\n",
    "    scaled = vs[0, start : start + width, :, :]\n",
    "    unscaled = vs[1, start : start + width, :, :]\n",
    "    return scaled, unscaled\n",
    "\n",
    "\n",
    "def flatten(v):\n",
    "    return np.array(list(zip(*sum([list(zip(*sl)) for sl in v], []))))\n",
    "\n",
    "\n",
    "def filter_slices(xy, ab, time_margin=time_margin, times=times, lower=None, upper=None):\n",
    "    if not lower:\n",
    "        lower = times[time_margin]\n",
    "    if not upper:\n",
    "        upper = times[-time_margin]\n",
    "    return ab.T[np.all([xy[1, :] > lower, xy[1, :] < upper], axis=0)].T\n",
    "\n",
    "\n",
    "def extract_points(\n",
    "    scaled,\n",
    "    unscaled,\n",
    "    times=times,\n",
    "    time_margin=time_margin,\n",
    "    beam_width=beam_width,\n",
    "    width=width,\n",
    "):\n",
    "    points = flatten(scaled)\n",
    "    points_raw = flatten(unscaled)\n",
    "    lin = np.linspace(0, np.max(points[0]), beam_width)\n",
    "    indices = np.tile(list(range(beam_width)), width)\n",
    "\n",
    "    indices = filter_slices(points, indices, time_margin=time_margin, times=times)\n",
    "    points_raw = filter_slices(points, points_raw, time_margin=time_margin, times=times)\n",
    "    points = filter_slices(points, points, time_margin=time_margin, times=times)\n",
    "    return lin, indices, points, points_raw\n",
    "\n",
    "\n",
    "if not imported:\n",
    "    scaled, unscaled = split_vs(vs)\n",
    "    lin, indices, points, points_raw = extract_points(scaled, unscaled)\n",
    "    # Average said events for a representation of event occurence\n",
    "    # within an entire PSB.\n",
    "    XY = np.mean(scaled, axis=0)\n",
    "    raw = np.mean(unscaled, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcS4Ygg4wuBf"
   },
   "outputs": [],
   "source": [
    "\"\"\"Extract the minimum events for an entire band.\"\"\"\n",
    "\n",
    "\n",
    "def extract_hull(\n",
    "    images, points, points_raw, indices, time_margin=time_margin, beam_width=beam_width\n",
    "):\n",
    "    hull = [\n",
    "        min(\n",
    "            list(\n",
    "                points[1][indices == i][\n",
    "                    points[1][indices == i] > list(images.keys())[time_margin + 1]\n",
    "                ]\n",
    "            )\n",
    "            + [list(images.keys())[-time_margin - 1]]\n",
    "        )\n",
    "        for i in range(beam_width)\n",
    "    ]\n",
    "    hull_raw = [\n",
    "        min(\n",
    "            list(\n",
    "                points_raw[1][indices == i][\n",
    "                    points_raw[1][indices == i] > time_margin + 1\n",
    "                ]\n",
    "            )\n",
    "            + [len(images) - time_margin - 1]\n",
    "        )\n",
    "        for i in range(beam_width)\n",
    "    ]\n",
    "    return hull, hull_raw\n",
    "\n",
    "\n",
    "if not imported:\n",
    "    hull, hull_raw = extract_hull(images, points, points_raw, indices, time_margin=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $imported\n",
    "# The velocity of a given nucleation should be slope^-1\n",
    "# A nucleation event should be early, so we expect we can see these as\n",
    "#\n",
    "#   \\     /\n",
    "#  S \\   / Q\n",
    "#     \\ /\n",
    "#      v\n",
    "#\n",
    "# Where S^-1 would be speed of the left front, and Q^-1, the speed of the\n",
    "# right front.\n",
    "#\n",
    "# It's worth noting, that mean may be incorrect. Each segment may have a\n",
    "# different length. Depending on where the PSB was chosen, you may have to\n",
    "# adjust for size considerations. However, with a small band they should only\n",
    "# vary slightly. So this provides a general idea regardless\n",
    "\n",
    "plt.imshow(np.mean(cube[:, start:start + width, :], axis=1))\n",
    "plt.plot(raw[0, :], np.array(hull_raw))\n",
    "plt.savefig(\"svg/band{}_{}.svg\".format(experiment, band))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.imshow(np.mean(cube[:, start:start + width, :], axis=1))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.imshow(np.std(cube[:, start:start + width, :], axis=1))\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.imshow(np.std(cube[:, start:start + width, :], axis=1))\n",
    "plt.plot(raw[0, :], hull_raw)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "plt.title(\"\"\"Position vs first event cycle for an entire band.\"\"\") \n",
    "plt.plot(points[0], points[1], \"oy\", zorder=-1)\n",
    "plt.plot(lin, hull)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "np.savetxt(\"csv/dump_{}_band_{}.csv\".format(experiment, band), np.array([lin, hull]).T, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"By using bucketing as a noise reduction technique, we ensure that the general shape of\n",
    "the profile is not compromised while reducing event detection mistakes.\"\"\"\n",
    "\n",
    "\n",
    "def make_smooth_profile(\n",
    "    bucket,\n",
    "    images,\n",
    "    scaled,\n",
    "    unscaled,\n",
    "    plotted=True,\n",
    "    times=times,\n",
    "    width=width,\n",
    "    experiment=experiment,\n",
    "    band=band,\n",
    "    time_margin=time_margin,\n",
    "    study_name=None,\n",
    "):\n",
    "    \"\"\"Groups all terms into 'bucket' sized groups and then performs aggregation.\"\"\"\n",
    "    if not study_name:\n",
    "        study_name = \"\"\n",
    "    scaled_temp = np.min(np.reshape(scaled, [width, 2, bucket, -1]), axis=-1)\n",
    "    unscaled_temp = np.min(np.reshape(unscaled, [width, 2, bucket, -1]), axis=-1)\n",
    "\n",
    "    points = flatten(scaled_temp)\n",
    "    points_raw = flatten(unscaled_temp)\n",
    "    lin = np.linspace(0, np.max(points[0]), bucket)\n",
    "    indices = np.tile(list(range(bucket)), width)\n",
    "\n",
    "    indices = filter_slices(points, indices, time_margin=time_margin, times=times)\n",
    "    points_raw = filter_slices(points, points_raw, time_margin=time_margin, times=times)\n",
    "    points = filter_slices(points, points, time_margin=time_margin, times=times)\n",
    "    points = np.array([points[1], points[0]])\n",
    "    indices = filter_slices(points, indices, upper=15, lower=2.5)\n",
    "    points = filter_slices(points, points, upper=15, lower=2.5)\n",
    "    points = np.array([points[1], points[0]])\n",
    "\n",
    "    hull = [\n",
    "        min(list(points[1][indices == i]) + [list(images.keys())[-time_margin]]) / 1e6\n",
    "        for i in range(bucket)\n",
    "    ]\n",
    "\n",
    "    np.savetxt(\n",
    "        \"csv/dump_{}_band_{}_bucket_{}{}.csv\".format(\n",
    "            experiment, band, beam_width // bucket, study_name\n",
    "        ),\n",
    "        np.array([lin, hull]).T,\n",
    "        delimiter=\",\",\n",
    "    )\n",
    "    if plotted:\n",
    "        plt.title(\n",
    "            \"\"\"Position vs cycle for grouped events (in regions of ± {:.0f} nm)\n",
    "    detected in a PSM band, and the extracted minimum profile.\"\"\".format(\n",
    "                1e3 * (beam_width // bucket) * (scale / lengths[start]) / 2\n",
    "            )\n",
    "        )\n",
    "        plt.plot(\n",
    "            points[0] - 2.5,\n",
    "            points[1] / 1e6,\n",
    "            \"oc\",\n",
    "            label=\"Location of event detected\",\n",
    "            zorder=-1,\n",
    "        )\n",
    "        plt.plot(\n",
    "            lin - 2.5, hull, label=\"Profile constructed from using first detected event\"\n",
    "        )\n",
    "        plt.ylabel(\"\"\"Cycles in 1e6 from begining of the experiment.\"\"\")\n",
    "        plt.xlabel(\"\"\"Micrometers relative to the left most point on the PSM.\"\"\")\n",
    "        plt.xlim(0, 12.5)\n",
    "        plt.ylim(0, 5)\n",
    "        plt.legend()\n",
    "        plt.savefig(\n",
    "            \"svg/band{}_{}__bucket_{}.svg\".format(\n",
    "                experiment, band, beam_width // bucket\n",
    "            )\n",
    "        )\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def factors(n):\n",
    "    \"\"\"A nice way to generate the factors of a number.\"\"\"\n",
    "    r = np.arange(1, int(n ** 0.5) + 1)\n",
    "    x = r[np.mod(n, r) == 0]\n",
    "    return list(sorted(np.concatenate((x, n // x), axis=None), reverse=True))\n",
    "\n",
    "\n",
    "def export_profiles(\n",
    "    images,\n",
    "    scaled,\n",
    "    unscaled,\n",
    "    plotted=True,\n",
    "    times=times,\n",
    "    beam_width=beam_width,\n",
    "    width=width,\n",
    "    experiment=experiment,\n",
    "    band=band,\n",
    "    time_margin=time_margin,\n",
    "    study_name=None,\n",
    "):\n",
    "    # The final 2 factors produce useless plots, thus we ignore them\n",
    "    for factor in factors(beam_width)[:-2]:\n",
    "        make_smooth_profile(\n",
    "            factor,\n",
    "            images,\n",
    "            scaled,\n",
    "            unscaled,\n",
    "            plotted=plotted,\n",
    "            times=times,\n",
    "            width=width,\n",
    "            experiment=experiment,\n",
    "            band=band,\n",
    "            time_margin=time_margin,\n",
    "            study_name=study_name,\n",
    "        )\n",
    "\n",
    "\n",
    "if not imported:\n",
    "    export_profiles(\n",
    "        images,\n",
    "        scaled,\n",
    "        unscaled,\n",
    "        plotted=True,\n",
    "        width=width,\n",
    "        experiment=experiment,\n",
    "        band=band,\n",
    "        time_margin=time_margin,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(profile, window_size=10):\n",
    "    scores = np.zeros(len(profile))\n",
    "    for i in range(window_size, len(profile) - window_size):\n",
    "        window = profile[i - window_size : i + window_size, 1]\n",
    "        scores[i - window_size : i + window_size] = (\n",
    "            window - window.mean()\n",
    "        ) / window.std()\n",
    "    return abs(scores)\n",
    "\n",
    "\n",
    "def cleaned_hull(lin, hull, left=2.5, right=16, confidence=1.645):\n",
    "    filtered = np.vstack([lin, hull])\n",
    "    filtered = (filtered[:, lin > left])[:, lin[lin > left] < right]\n",
    "    filtered = filtered.T\n",
    "    filtered = filtered[remove_noise(filtered) < confidence]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "if interactive and not imported:\n",
    "    filtered = cleaned_hull(lin, hull)\n",
    "    plt.plot(*list(zip(*convex_hull(filtered))))\n",
    "    plt.plot(*list(zip(*filtered)))\n",
    "    print(score_profile(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFhzM9xhzvOw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Extract a time/displacement graph for a segment in the profile.\"\"\"\n",
    "\n",
    "\n",
    "def filter_segment(xy, ab, lower=5, upper=8):\n",
    "    ab = ab[:, np.all([xy[0, :] > lower, xy[0, :] < upper], axis=0)]\n",
    "    return ab\n",
    "\n",
    "\n",
    "def fit_segment(xy, lower=5, upper=8, right=False, preserve=False, plot=True):\n",
    "    # Crop to the segement of the band\n",
    "    xy = filter_segment(xy, xy, lower=lower, upper=upper)\n",
    "\n",
    "    npmax, npmin = np.max, np.min\n",
    "    if right:\n",
    "        npmax, npmin = np.min, np.max\n",
    "\n",
    "    # Adjust so t=0 is nucleation event\n",
    "    if not preserve:\n",
    "        xy[1, :] -= np.min(xy[1, :])\n",
    "\n",
    "    # Adjust so displacement is positivly relative to displacement\n",
    "    if not preserve:\n",
    "        xy[0, :] -= npmax(xy[0, :])\n",
    "\n",
    "    if not right and not preserve:\n",
    "        xy[0, :] *= -1\n",
    "\n",
    "    # Swap, because displacement time graph, should be displacement v time.\n",
    "    xy = np.array([xy[1, :], xy[0, :]])\n",
    "\n",
    "    # Provide a linear fit.\n",
    "    fit = np.polyfit(xy[0, :], xy[1, :], 1)\n",
    "    fit_fn = np.poly1d(fit)\n",
    "\n",
    "    M = np.max(xy[0, :])\n",
    "    X = np.arange(0, M, M / 48)\n",
    "    if plot:\n",
    "        plt.title(\"Displacement/Velocity for {}, band {}.\".format(experiment, band))\n",
    "        plt.xlabel(\"Cycles since start of Nucleation\")\n",
    "        plt.ylabel(u\"Propogation distance from Nucleation site (μm).\")\n",
    "        plt.plot(xy[0, :], xy[1, :], \"bo\", X, fit_fn(X), \"--k\")\n",
    "        plt.savefig(\"svg/speed{}_{}.svg\".format(experiment, band))\n",
    "        plt.show()\n",
    "    return scipy.stats.linregress(*xy)\n",
    "\n",
    "\n",
    "if not imported:\n",
    "    line = fit_segment(np.array([lin, hull]), lower=lower, upper=upper, right=False)\n",
    "    print(line.slope, line.intercept, line.rvalue, line.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Determine a likely embryo from the fit of the profile to randomness.\"\"\"\n",
    "\n",
    "\n",
    "def embryo(xy, dx=0.1, lower=2.5, upper=15, debug=False, plot=True):\n",
    "    \"\"\"Determines embyronic width of a profile.\n",
    "  Starting with the lowest point on the profile, determines if a\n",
    "  collection of points is likely random by comparion to gassian distribution.\n",
    "  The collection is then expanded by the surrounding lowest points.\n",
    "  \n",
    "  Args:\n",
    "      xy: The profile to consider\n",
    "      dx: The gradation step to take when expanding front\n",
    "      lower: The lower bound on the profile to be considered.\n",
    "      upper: The upper bound on the profile to be considered.\n",
    "      debug: Produce additional plots and logs\n",
    "      plot: Produce plot for randomness metrics\n",
    "\n",
    "  Returns:\n",
    "      tuple of embyro start, embyro end, embryo length, standard deviation, r^2 in embryo\n",
    "  \"\"\"\n",
    "    gradations = np.arange(dx, 100, dx)\n",
    "    xy = filter_segment(xy, xy, lower=lower, upper=upper)\n",
    "    line_xy = filter_segment(xy, xy, lower=5, upper=12)\n",
    "    xy = np.array([xy[1, :], xy[0, :]])\n",
    "    line_xy = np.array([line_xy[1, :], line_xy[0, :]])\n",
    "    line = fit_segment(line_xy, lower=0, upper=np.max(line_xy), plot=debug)\n",
    "    if debug:\n",
    "        print(line, np.min(xy[0, :]))\n",
    "    pps = None\n",
    "    if debug:\n",
    "        pps = pylab\n",
    "\n",
    "    def f(e):\n",
    "        gaussian = filter_segment(\n",
    "            xy, xy, lower=0, upper=np.min(xy[0, :]) + line.stderr * e\n",
    "        )\n",
    "        _, (c, i, rSquared) = probplot(gaussian[0, :], dist=\"norm\", plot=pps)\n",
    "        return rSquared\n",
    "\n",
    "    rs = np.array(list(map(f, gradations)))\n",
    "    selection = np.argmax(rs)\n",
    "    if plot:\n",
    "        plt.show()\n",
    "        plt.title(\n",
    "            \"\"\"Randomness Metric\n",
    "(negative log of 1 - r^2 from P-P plot)\n",
    "over profile.\"\"\"\n",
    "        )\n",
    "        plt.ylabel(\"Randomness metric\")\n",
    "        plt.xlabel(\"% of profile considered\")\n",
    "        plt.plot(gradations, -np.log(1 - rs))\n",
    "        plt.savefig(\"svg/random{}_{}.svg\".format(experiment, band))\n",
    "        plt.show()\n",
    "        plt.title(\"r^2 value of potential embryo vs. Gaussian distribution\")\n",
    "        plt.ylabel(\"r^2\")\n",
    "        plt.xlabel(\"% of profile considered\")\n",
    "        plt.plot(gradations, rs)\n",
    "        plt.show()\n",
    "        np.savetxt(\n",
    "            \"csv/randomraw_{}_band_{}.csv\".format(experiment, band),\n",
    "            np.array([gradations, rs]).T,\n",
    "            delimiter=\",\",\n",
    "        )\n",
    "        print(selection)\n",
    "\n",
    "    std = gradations[selection] * line.stderr\n",
    "    f(gradations[np.argmax(rs)])\n",
    "    filt = filter_segment(xy, xy, lower=0, upper=np.min(xy[0, :]) + std)[1, :]\n",
    "    plt.close(\"all\")\n",
    "    return np.min(filt), np.max(filt), np.max(filt) - np.min(filt), std, np.max(rs)\n",
    "\n",
    "\n",
    "if not imported:\n",
    "    print(\n",
    "        \"\"\"{2:.2f} micrometer long embyro spanning from position {0:.2f} to {1:.2f},\n",
    "correlated to Gaussian noise with an r^2 of {4:.3f}\"\"\".format(\n",
    "            *embryo(np.array([lin, hull]), lower=2.5, upper=16)\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PSM_Processing.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
